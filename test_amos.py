import os
import argparse
import torch
import torch.nn.functional as F
import numpy as np
import SimpleITK as sitk
from scipy.ndimage import zoom
from tqdm import tqdm
from math import ceil

# ç¡®ä¿å¯ä»¥ä»ä½ çš„é¡¹ç›®ç»“æ„ä¸­å¯¼å…¥ VNet å’Œ Config
# ä¾‹å¦‚: from networks.vnet import VNet
# from utils.config import Config
from networks.vnet import VNet
from networks.vnet_skcdf import VNet_Decouple_Attention_ABC
from networks.vnet_dst import VNet_Decoupled # æ›¿æ¢ä¸ºè§£è€¦æ¨¡å‹
from networks.vnet_dycon import VNet_dycon
from utils.config import Config

def sliding_window_inference(image, model, patch_size, num_classes, overlap=0.5, device='cuda',exp='dycon'):
    """
    ä½¿ç”¨æ»‘åŠ¨çª—å£å¯¹æ•´ä¸ª3Då›¾åƒè¿›è¡Œæ¨ç†ã€‚
    
    [V2 æ›´æ–°]:
    - å¦‚æœå›¾åƒå°ºå¯¸å°äºpatch_sizeï¼Œä¼šè‡ªåŠ¨å¯¹å›¾åƒè¿›è¡Œé›¶å¡«å……ï¼ˆpaddingï¼‰ã€‚
    - æ”¹è¿›äº†å¾ªç¯é€»è¾‘ï¼Œç¡®ä¿å›¾åƒçš„è¾¹ç¼˜å’Œè§’è½èƒ½è¢«å®Œæ•´è¦†ç›–ã€‚
    - æ¨ç†ç»“æŸåï¼Œä¼šè‡ªåŠ¨å°†å¡«å……åŒºåŸŸè£å‰ªæ‰ï¼Œè¿”å›ä¸åŸå›¾å¤§å°ä¸€è‡´çš„ç»“æœã€‚
    """
    # 1. è·å–åŸå§‹å›¾åƒå°ºå¯¸å’Œpatchå°ºå¯¸
    B, C, D, H, W = image.shape
    patch_d, patch_h, patch_w = patch_size

    # 2. è®¡ç®—å¹¶åº”ç”¨å¡«å……
    pad_d = max(0, patch_d - D)
    pad_h = max(0, patch_h - H)
    pad_w = max(0, patch_w - W)
    
    if pad_d > 0 or pad_h > 0 or pad_w > 0:
        # F.padçš„å¡«å……é¡ºåºæ˜¯ (W_left, W_right, H_top, H_bottom, D_front, D_back)
        image = F.pad(image, (0, pad_w, 0, pad_h, 0, pad_d), mode='constant', value=0)

    # è·å–å¡«å……åå›¾åƒçš„å°ºå¯¸
    padded_D, padded_H, padded_W = image.shape[2:]

    # 3. åˆå§‹åŒ–è¾“å‡ºå’Œè®¡æ•°å¼ é‡
    # num_classes = model.n_classes
    prediction_map = torch.zeros((1, num_classes, padded_D, padded_H, padded_W), device=device)
    count_map = torch.zeros((1, 1, padded_D, padded_H, padded_W), device=device)

    # 4. è®¡ç®—æ­¥é•¿å¹¶è¿›è¡Œæ»‘åŠ¨çª—å£æ¨ç†
    stride_d = int(patch_d * (1 - overlap)) if patch_d > 1 else 1
    stride_h = int(patch_h * (1 - overlap)) if patch_h > 1 else 1
    stride_w = int(patch_w * (1 - overlap)) if patch_w > 1 else 1
    
    # ç¡®ä¿æ­¥é•¿è‡³å°‘ä¸º1
    stride_d, stride_h, stride_w = max(1, stride_d), max(1, stride_h), max(1, stride_w)

    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ»‘åŠ¨æ¬¡æ•°ï¼Œç¡®ä¿è¦†ç›–åˆ°è¾¹ç¼˜
    steps_d = ceil((padded_D - patch_d) / stride_d) + 1 if padded_D > patch_d else 1
    steps_h = ceil((padded_H - patch_h) / stride_h) + 1 if padded_H > patch_h else 1
    steps_w = ceil((padded_W - patch_w) / stride_w) + 1 if padded_W > patch_w else 1

    for i_d in range(steps_d):
        # ç¡®ä¿æœ€åä¸€ä¸ªçª—å£çš„èµ·å§‹ä½ç½®å¯¹é½åˆ°å›¾åƒè¾¹ç¼˜
        d = min(stride_d * i_d, padded_D - patch_d)
        for i_h in range(steps_h):
            h = min(stride_h * i_h, padded_H - patch_h)
            for i_w in range(steps_w):
                w = min(stride_w * i_w, padded_W - patch_w)
                
                d_end, h_end, w_end = d + patch_d, h + patch_h, w + patch_w
                image_patch = image[:, :, d:d_end, h:h_end, w:w_end]
                
                with torch.no_grad():
                    if exp =='dycon':
                        _,outputs, _ = model(image_patch)
                    else:
                        outputs = model(image_patch)
                    outputs_softmax = F.softmax(outputs, dim=1)
                
                prediction_map[:, :, d:d_end, h:h_end, w:w_end] += outputs_softmax
                count_map[:, :, d:d_end, h:h_end, w:w_end] += 1
    
    # 5. å¹³å‡é‡å åŒºåŸŸçš„é¢„æµ‹
    prediction_map /= (count_map + 1e-8)
    prediction_padded = torch.argmax(prediction_map, dim=1).squeeze(0)

    # 6. è£å‰ªæ‰å¡«å……åŒºåŸŸï¼Œæ¢å¤åˆ°åŸå§‹å°ºå¯¸
    final_prediction = prediction_padded[:D, :H, :W]
    
    return final_prediction

def sliding_window_inference_skcdf(image, model, patch_size, num_classes, overlap=0.5, device='cuda'):
    """
    ä½¿ç”¨æ»‘åŠ¨çª—å£å¯¹æ•´ä¸ª3Då›¾åƒè¿›è¡Œæ¨ç†ã€‚
    
    [V2 æ›´æ–°]:
    - å¦‚æœå›¾åƒå°ºå¯¸å°äºpatch_sizeï¼Œä¼šè‡ªåŠ¨å¯¹å›¾åƒè¿›è¡Œé›¶å¡«å……ï¼ˆpaddingï¼‰ã€‚
    - æ”¹è¿›äº†å¾ªç¯é€»è¾‘ï¼Œç¡®ä¿å›¾åƒçš„è¾¹ç¼˜å’Œè§’è½èƒ½è¢«å®Œæ•´è¦†ç›–ã€‚
    - æ¨ç†ç»“æŸåï¼Œä¼šè‡ªåŠ¨å°†å¡«å……åŒºåŸŸè£å‰ªæ‰ï¼Œè¿”å›ä¸åŸå›¾å¤§å°ä¸€è‡´çš„ç»“æœã€‚
    """
    # 1. è·å–åŸå§‹å›¾åƒå°ºå¯¸å’Œpatchå°ºå¯¸
    B, C, D, H, W = image.shape
    patch_d, patch_h, patch_w = patch_size

    # 2. è®¡ç®—å¹¶åº”ç”¨å¡«å……
    pad_d = max(0, patch_d - D)
    pad_h = max(0, patch_h - H)
    pad_w = max(0, patch_w - W)
    
    if pad_d > 0 or pad_h > 0 or pad_w > 0:
        # F.padçš„å¡«å……é¡ºåºæ˜¯ (W_left, W_right, H_top, H_bottom, D_front, D_back)
        image = F.pad(image, (0, pad_w, 0, pad_h, 0, pad_d), mode='constant', value=0)

    # è·å–å¡«å……åå›¾åƒçš„å°ºå¯¸
    padded_D, padded_H, padded_W = image.shape[2:]

    # 3. åˆå§‹åŒ–è¾“å‡ºå’Œè®¡æ•°å¼ é‡
    # num_classes = model.n_classes
    prediction_map = torch.zeros((1, num_classes, padded_D, padded_H, padded_W), device=device)
    count_map = torch.zeros((1, 1, padded_D, padded_H, padded_W), device=device)

    # 4. è®¡ç®—æ­¥é•¿å¹¶è¿›è¡Œæ»‘åŠ¨çª—å£æ¨ç†
    stride_d = int(patch_d * (1 - overlap)) if patch_d > 1 else 1
    stride_h = int(patch_h * (1 - overlap)) if patch_h > 1 else 1
    stride_w = int(patch_w * (1 - overlap)) if patch_w > 1 else 1
    
    # ç¡®ä¿æ­¥é•¿è‡³å°‘ä¸º1
    stride_d, stride_h, stride_w = max(1, stride_d), max(1, stride_h), max(1, stride_w)

    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ»‘åŠ¨æ¬¡æ•°ï¼Œç¡®ä¿è¦†ç›–åˆ°è¾¹ç¼˜
    steps_d = ceil((padded_D - patch_d) / stride_d) + 1 if padded_D > patch_d else 1
    steps_h = ceil((padded_H - patch_h) / stride_h) + 1 if padded_H > patch_h else 1
    steps_w = ceil((padded_W - patch_w) / stride_w) + 1 if padded_W > patch_w else 1

    for i_d in range(steps_d):
        # ç¡®ä¿æœ€åä¸€ä¸ªçª—å£çš„èµ·å§‹ä½ç½®å¯¹é½åˆ°å›¾åƒè¾¹ç¼˜
        d = min(stride_d * i_d, padded_D - patch_d)
        for i_h in range(steps_h):
            h = min(stride_h * i_h, padded_H - patch_h)
            for i_w in range(steps_w):
                w = min(stride_w * i_w, padded_W - patch_w)
                
                d_end, h_end, w_end = d + patch_d, h + patch_h, w + patch_w
                image_patch = image[:, :, d:d_end, h:h_end, w:w_end]
                
                with torch.no_grad():
                    outputs,_ = model(image_patch, pred_type = "unlabeled")
                    outputs_softmax = F.softmax(outputs, dim=1)
                
                prediction_map[:, :, d:d_end, h:h_end, w:w_end] += outputs_softmax
                count_map[:, :, d:d_end, h:h_end, w:w_end] += 1
    
    # 5. å¹³å‡é‡å åŒºåŸŸçš„é¢„æµ‹
    prediction_map /= (count_map + 1e-8)
    prediction_padded = torch.argmax(prediction_map, dim=1).squeeze(0)

    # 6. è£å‰ªæ‰å¡«å……åŒºåŸŸï¼Œæ¢å¤åˆ°åŸå§‹å°ºå¯¸
    final_prediction = prediction_padded[:D, :H, :W]
    
    return final_prediction

class ModelEnsemble(torch.nn.Module):
    def __init__(self, model_A, model_B):
        super(ModelEnsemble, self).__init__()
        self.model_A = model_A
        self.model_B = model_B

    def forward(self, x):
        return (self.model_A(x) + self.model_B(x)) / 2.0

class ModelEnsemble_slc(torch.nn.Module):
    def __init__(self, model_A, model_B):
        super(ModelEnsemble_slc, self).__init__(); self.model_A = model_A; self.model_B = model_B
    def forward(self, x):
        out_A = self.model_A(x); out_soft_A = F.softmax(out_A, dim=1)
        # [å¿ å®å®ç°] éªŒè¯æ—¶ä¹Ÿä½¿ç”¨ 1 - soft_A ä½œä¸ºè¾“å…¥
        in_B = torch.cat([x, 1 - out_soft_A], dim=1)
        out_B = self.model_B(in_B)
        return (out_soft_A + F.softmax(out_B, dim=1)) / 2.0

def main(args):
    # --- 1. è®¾ç½®å’ŒåŠ è½½é…ç½® ---
    print("âœ¨ 1. Loading configuration and model...")
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    config = Config(args.task)
    patch_size = config.patch_size
    
    if args.exp =='uamt':
        model = VNet(
        n_channels=config.num_channels,
        n_classes=config.num_cls,
        n_filters=config.n_filters,
        normalization='batchnorm',
        has_dropout=False  # æ¨ç†æ—¶å…³é—­ dropout
        ).to(device)

        # b. åŠ è½½æƒé‡ (å…³é”®ä¿®æ”¹: åŠ è½½ ema_model çš„æƒé‡)
        checkpoint = torch.load(args.model_path)
        # æ ¹æ®ä½ çš„è®­ç»ƒä»£ç ï¼Œæƒé‡æ˜¯ä¿å­˜åœ¨ 'ema_model' é”®ä¸‹çš„
        model.load_state_dict(checkpoint['ema_model'])  # <-- æ ¸å¿ƒä¿®æ”¹
        model.eval()

        print(f"âœ… UAMT Teacher (EMA) model loaded from {args.model_path}")
    elif args.exp =='slcnet':
        model1 = VNet(
        n_channels=config.num_channels, n_classes=config.num_cls, n_filters=config.n_filters,
        normalization='batchnorm', has_dropout=False).cuda()
        model2 = VNet(
        n_channels=1 + config.num_cls,
        n_classes=config.num_cls,
        n_filters=config.n_filters,
        normalization='batchnorm',
        has_dropout=False
        ).cuda()

        # b. åŠ è½½æƒé‡
        checkpoint = torch.load(args.model_path)
        model1.load_state_dict(checkpoint['A'])
        model2.load_state_dict(checkpoint['B'])
        model1.eval()
        model2.eval()
        model = ModelEnsemble_slc(model1, model2)
    elif args.exp =='skcdf':
        model = VNet_Decouple_Attention_ABC(
        n_channels=config.num_channels, n_classes=config.num_cls, n_filters=config.n_filters,
        normalization='batchnorm', has_dropout=True
    ).cuda()
        checkpoint = torch.load(args.model_path)
        model.load_state_dict(checkpoint['A'])
        model.eval()
    elif args.exp =='dst':
        model_A = VNet_Decoupled(
        n_channels=config.num_channels,
        n_classes=config.num_cls,
        n_filters=config.n_filters,
        normalization='batchnorm',
        has_dropout=False
        ).to(device)
        model_B = VNet_Decoupled(
            n_channels=config.num_channels,
            n_classes=config.num_cls,
            n_filters=config.n_filters,
            normalization='batchnorm',
            has_dropout=False
        ).to(device)

        # b. åŠ è½½æƒé‡
        checkpoint = torch.load(args.model_path)
        model_A.load_state_dict(checkpoint['A'])
        model_B.load_state_dict(checkpoint['B'])
        model_A.eval()
        model_B.eval()

        # c. åˆ›å»ºç”¨äºæ¨ç†çš„é›†æˆæ¨¡å‹
        model = ModelEnsemble(model_A, model_B) # è¿™é‡Œçš„ model ä¼šè¢«ä¼ é€’ç»™ sliding_window_inference
        print(f"âœ… DST ensemble model loaded from {args.model_path}")
    elif args.exp =='dycon':
        model = VNet_dycon(
        n_channels=config.num_channels,
        n_classes=config.num_cls,
        n_filters=config.n_filters,
        normalization='batchnorm',
        has_dropout=False
        ).to(device)

        # b. åŠ è½½æƒé‡
        checkpoint = torch.load(args.model_path)
        model.load_state_dict(checkpoint['ema_model'])
        model.eval()

        # c. åˆ›å»ºç”¨äºæ¨ç†çš„é›†æˆæ¨¡å‹
        print(f"âœ… Dycon ensemble model loaded from {args.model_path}")
    else:
        # --- 2. åŠ è½½æ¨¡å‹ ---
        model_A = VNet(
        n_channels=config.num_channels,
        n_classes=config.num_cls,
        n_filters=config.n_filters,
        normalization='batchnorm',
        has_dropout=False
        ).to(device)
        model_B = VNet(
            n_channels=config.num_channels,
            n_classes=config.num_cls,
            n_filters=config.n_filters,
            normalization='batchnorm',
            has_dropout=False
        ).to(device)

        # b. åŠ è½½æƒé‡
        checkpoint = torch.load(args.model_path)
        model_A.load_state_dict(checkpoint['A'])
        model_B.load_state_dict(checkpoint['B'])
        model_A.eval()
        model_B.eval()

        # c. åˆ›å»ºç”¨äºæ¨ç†çš„é›†æˆæ¨¡å‹
        model = ModelEnsemble(model_A, model_B) # è¿™é‡Œçš„ model ä¼šè¢«ä¼ é€’ç»™ sliding_window_inference
    print(f"âœ… SEMI-SUPERVISED ensemble model loaded from {args.model_path}")

    # --- 3. åˆ›å»ºè¾“å‡ºç›®å½• ---
    os.makedirs(args.output_path, exist_ok=True)

    # --- 4. éå†NPYæ–‡ä»¶å¹¶è¿›è¡Œæ¨ç† ---
    npy_files = [f for f in os.listdir(args.npy_path) if f.endswith('.npy')]
    print(f"ğŸš€ 4. Found {len(npy_files)} .npy files. Starting inference...")

    for npy_filename in tqdm(npy_files, desc="Inference Progress"):
        # --- a. æ„å»ºæ–‡ä»¶è·¯å¾„å¹¶æ£€æŸ¥ ---
        nii_filename = npy_filename.replace('_image.npy', '.nii.gz')
        original_nii_filepath = os.path.join(args.original_nii_path, nii_filename)
        npy_filepath = os.path.join(args.npy_path, npy_filename)

        if not os.path.exists(original_nii_filepath):
            print(f"âš ï¸ Warning: Corresponding file {nii_filename} not found in {args.original_nii_path}. Skipping {npy_filename}.")
            continue

        # --- b. ä».nii.gzæ–‡ä»¶åŠ è½½åŸå§‹å…ƒæ•°æ® ---
        sitk_image_orig = sitk.ReadImage(original_nii_filepath)
        original_spacing = sitk_image_orig.GetSpacing()
        original_size = sitk_image_orig.GetSize() # (x, y, z)
        original_origin = sitk_image_orig.GetOrigin()
        original_direction = sitk_image_orig.GetDirection()

        # --- c. ä».npyæ–‡ä»¶åŠ è½½æ•°æ®å¹¶é¢„å¤„ç† ---
        # 1. åŠ è½½å·²ç»é‡é‡‡æ ·ï¼ˆresampledï¼‰çš„å›¾åƒæ•°æ®
        image_np = np.load(npy_filepath).astype(np.float32)

        # 2. å½’ä¸€åŒ– (å‡è®¾.npyæ–‡ä»¶åªç»è¿‡é‡é‡‡æ ·ï¼Œæœªè¿›è¡Œå½’ä¸€åŒ–)
        # å¦‚æœä½ çš„.npyæ–‡ä»¶å·²ç»å½’ä¸€åŒ–ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ
        image_np = image_np.clip(min=-125, max=275)
        image_np = (image_np + 125) / 400
        # image_np = (image_np - np.mean(image_np)) / np.std(image_np)

        # 3. è½¬æ¢ä¸º Tensor
        image_tensor = torch.from_numpy(image_np).unsqueeze(0).unsqueeze(0) # (1, 1, D, H, W)
        image_tensor = image_tensor.to(device)

        if args.exp =='skcdf':
            prediction_tensor = sliding_window_inference_skcdf(
            image=image_tensor,
            model=model,
            patch_size=patch_size,
            num_classes=config.num_cls,
            overlap=args.overlap
        )
        elif args.exp =='dycon':
            prediction_tensor = sliding_window_inference(
                image=image_tensor,
                model=model,
                patch_size=patch_size,
                num_classes=config.num_cls,
                overlap=args.overlap,
                exp = args.exp
            )
        else:
            # --- d. æ»‘åŠ¨çª—å£æ¨ç† ---
            prediction_tensor = sliding_window_inference(
                image=image_tensor,
                model=model,
                patch_size=patch_size,
                num_classes=config.num_cls,
                overlap=args.overlap,
                exp = args.exp
            )
        prediction_np = prediction_tensor.cpu().numpy().astype(np.uint8) # (D, H, W)
        # --- e. åå¤„ç†ï¼šé‡é‡‡æ ·å›åŸå§‹å°ºå¯¸ ---
        # é¢„æµ‹ç»“æœçš„å°ºå¯¸ä¸.npyæ–‡ä»¶çš„å°ºå¯¸ä¸€è‡´
        # æˆ‘ä»¬éœ€è¦å°†å…¶æ¢å¤åˆ°åŸå§‹.nii.gzæ–‡ä»¶çš„å°ºå¯¸
        original_size_np_order = (original_size[2], original_size[1], original_size[0]) # (z, y, x)
        
        resample_back_factor = [
            original_size_np_order[i] / prediction_np.shape[i] for i in range(3)
        ]
        
        # ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼(order=0)æ¥é‡é‡‡æ ·åˆ†å‰²æ©ç 
        resampled_prediction_np = zoom(prediction_np, resample_back_factor, order=0, mode='nearest')

        # --- f. ä¿å­˜ä¸º.nii.gzæ–‡ä»¶ï¼Œå¹¶æ¢å¤å…ƒæ•°æ® ---
        prediction_sitk = sitk.GetImageFromArray(resampled_prediction_np)
        
        prediction_sitk.SetSpacing(original_spacing)
        prediction_sitk.SetOrigin(original_origin)
        prediction_sitk.SetDirection(original_direction)
        
        output_filepath = os.path.join(args.output_path, nii_filename)
        sitk.WriteImage(prediction_sitk, output_filepath)

    print("ğŸ‰ All predictions are saved successfully!")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="3D Medical Image Segmentation Inference from .npy files")
    
    # --- ä¿®æ”¹äº†è¿™é‡Œçš„è·¯å¾„å‚æ•° ---
    parser.add_argument('--npy_path', type=str, help="Path to the directory containing pre-processed .npy files.", default='/data/wangbo/CissMOS/Datasets/AMOS22_1.5_2.0_npy/imagesTr')
    parser.add_argument('--original_nii_path', type=str, help="Path to the directory with original .nii.gz files for metadata.", default='/data/wangbo/CissMOS/Datasets/Amos22/imagesTr')
    parser.add_argument('--output_path', type=str, help="Path to save the segmentation results.", default='/data/wangbo/CissMOS/training_set_results')
    parser.add_argument('--model_path', type=str, help="Path to the trained model checkpoint (.pth file).", default='/data/wangbo/CissMOS/logs/amos/uncertainty_driven_sampling/disagreement_snapshot/seed_1/ckpts/best_model.pth')
    
    # --- å…¶ä»–å‚æ•°ä¿æŒä¸å˜ ---
    parser.add_argument('--exp', type=str, default='uamt', help="GPU ID to use.")
    parser.add_argument('-g', '--gpu', type=str, default='0', help="GPU ID to use.")
    parser.add_argument('--task', type=str, default='amos', help="Task name to load the correct configuration (e.g., 'amos').")
    parser.add_argument('--overlap', type=float, default=0.5, help="Overlap ratio for sliding window, between 0 and 1.")
    
    args = parser.parse_args()
    
    main(args)